{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGu-avAzS48b"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZIukxdbQbjY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# Add any other packages you would like to use here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0l5OEtrS2ww"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "* The label in the dataset is given as `is_canceled`.\n",
    "* For a complete description of dataset, visit the link: https://www.sciencedirect.com/science/article/pii/S2352340918315191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "Zpih7K5PRm5h",
    "outputId": "0b3ed19e-173d-4a1c-cbcd-2454a8464e52"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/data/train/hotel_bookings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tTaFJE5WThl"
   },
   "source": [
    " ## Helpful EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVodDz_0aFu8",
    "outputId": "7135f791-6a46-4027-c588-a86d053aeab5"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Epol_f85Zt1V",
    "outputId": "db3da042-4f16-4357-d414-def181e29313"
   },
   "outputs": [],
   "source": [
    "df['reservation_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwx0-5mDdaea",
    "outputId": "91c22dbe-b66d-43f7-82cf-8bd888febb2c"
   },
   "outputs": [],
   "source": [
    "df['is_canceled'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZb-XtBrWEAF",
    "outputId": "a6e2cd6a-d75e-406b-cdb9-128dffea1f4c"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As written I will keep this part simple\n",
    "\n",
    "At first glance, the dataset looks very clean, even surprisingly clean (benchmark dataset).\n",
    "\n",
    "According to the assignment, I will solve a classification task and will use regression trees, which have proven to be the most robust tool in practice. Where can I help myself with the `sklearn` package, where the complete handling is already programmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates().shape[0]\n",
    "# a lot duplicates are questionable. I cant see any ID column what indicates records.\n",
    "# for this purpose and short time I gonna use only unique rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "if it is a classification task, I am most interested in how I have balanced data between cancel and non-cancel. Plus cross-correlation. Then I will be interested in how to impute valuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = list(df.select_dtypes(include='object').columns)\n",
    "numeric_columns = list(df.select_dtypes(exclude='object').columns)\n",
    "print(\"Object Columns:\", object_columns)\n",
    "print(\"\\nNumeric Columns:\", numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_hist(df:pd.DataFrame, column:str):\n",
    "    string_column = df[column]\n",
    "    value_counts = string_column.value_counts()\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    value_counts.plot(kind='bar')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in object_columns:\n",
    "    print(col)\n",
    "    plot_hist(df, col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalance in data, such as waiting for something like food, country, and so on. A decision tree could help with that.\n",
    "\n",
    "distribution_channel is a subset of market_segment, and this could introduce uncertainty for the model.\n",
    "\n",
    "Similarly, reserved_room_type and assigned_room_type seem identical at first glance.\n",
    "\n",
    "reservation_status is particularly interesting. I see 'canceled' there, which I would assume will be strongly correlated with a canceled response.\n",
    "\n",
    "Some predictors also contain NaNs. In this case, I would impute them as unknown. It could harm the model if I were to use the most common value, as these unknowns may carry information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df[\"reserved_room_type\"] != df[\"assigned_room_type\"])\n",
    "# not same only in 15k cases lets make new variable named reserved_assignet_diff\n",
    "# this will distinguish between them. or for shake of simplicity not count with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[df[\"reserved_room_type\"] != df[\"assigned_room_type\"]]\n",
    "sub_df[sub_df[\"is_canceled\"] == 1][[\"reserved_room_type\", \"assigned_room_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"reservation_status\"] == \"Canceled\"][\"is_canceled\"].unique() \n",
    "# this gonna be the strongest predictor ever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df[\"is_canceled\"]) - df[df[\"reservation_status\"] == \"Canceled\"].shape[0]\n",
    "# not sure what I should predict here. there is two ways:\n",
    "# - use full dataset and let model learn from alredy canceled cases to predict cancelation\n",
    "#   in general.\n",
    "# - split data and learn model only on not canceled data, this will lead model to learn\n",
    "#   patterns after cancelation.\n",
    "# customer can cancel reseravation shortly after reservation and this behaviour will \n",
    "# bias model we need to develop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"company\"].unique() # weird company... i will not use this one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() #Â lets check for hard unusual observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I expect the responce is highly correlated with lead_time, previous_cancellations,\n",
    "# booking_changes, required_car_parking_spaces and total_of_special_requests.\n",
    "# Between regressors above are strong correlation too. What si not good for trees.\n",
    "\n",
    "# I am bit nervous from strong correlation previous_bookings_not_canceled vs. \n",
    "# is_repeated_guest, both looks as good predictors, hmm somehow merge it.\n",
    "\n",
    "# required_car_parking_spaces another \"clear\" predictor. Person who book a hotel and forgot \n",
    "# to check if they have a parking, than realize they not... \n",
    "\n",
    "# similar story for total_of_special_requests, booking_changes etc.\n",
    "\n",
    "# in general I dont need all columns for cold start, I will cherry pick someones to \n",
    "# avoid multicolinearity. Droped predictors are mostly suspicious or unlogic for such \n",
    "# modelling (like meal, arrival_date_year). For next steps I propose to do deeper dive into.\n",
    "\n",
    "# also I will close my eyes to extreme values\n",
    "\n",
    "# of course if we want predict future arrival_date_year is not a good to extrapolate (experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(X.select_dtypes(include='object').columns)\n",
    "numerical_features = list(X.select_dtypes(exclude='object').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stays_in_nights\"] = df[\"stays_in_weekend_nights\"] + df[\"stays_in_week_nights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['hotel', 'arrival_date_month', 'market_segment', 'deposit_type', 'customer_type']\n",
    "numerical_features = ['lead_time', 'stays_in_nights', 'adults', 'is_repeated_guest', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'agent', 'required_car_parking_spaces', 'total_of_special_requests']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "for training part I gonna use sklearn pipelines, I have good experiences as good benchmark. As next step I propose to experiment with torch MLP \n",
    "\n",
    "I drop a lot features what looks suspicious or doesnt fit to my instincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "X = df.drop(columns=[\"is_canceled\"])\n",
    "y = df[\"is_canceled\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"unknown\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid to search\n",
    "# in production this grid search is not neccesary\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "    'xgb__learning_rate': [0.01, 0.2, 0.5],  # Step size shrinkage used to prevent overfitting\n",
    "    'xgb__max_depth': [3, 5, 10],  # Maximum depth of a tree\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(xgb_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:\", grid_search.best_score_)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added reservation_status and result are not supprising, I got a best model ever with\n",
    "# 100% accuracy but this is not what we want.\n",
    "#Best Parameters: {'xgb__learning_rate': 0.01, 'xgb__max_depth': 3, 'xgb__n_estimators': 50}\n",
    "#Best CV Score: 1.0\n",
    "#Accuracy: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
